# GPT, ChatGPT, InstructGPT
![image](https://user-images.githubusercontent.com/129742046/236900096-c38791b3-498e-4abb-85b4-277924d4a3bd.png)

![image](https://user-images.githubusercontent.com/129742046/236911422-d85a9d3e-ea46-4cf7-9fa1-dac4c0e3b4d3.png)




### 1. ChatGPT

- ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.
- Primarily trained using unsupervised learning - Pretraining, is unsupervised, where the model learns from a large corpus of text to understand language patterns and generate coherent responses. 
- fine-tuned using Supervised Learning + RLHF 
- **Dataset :** A massive corpus of text data, around 570GB of data sourced from books, wikipedia, research articles, webtexts, websites and other forms of content and writing on the net - Approximately 300 billion words were fed into the system.
- The model works on probability as a result of which it is able to predict the next word/prompt in a sentence.
- While training, if the model gets output wrong, the correct answer is fed back into the model thereby training it to the right responses and also helping it build on its knowledge bank.
- It then goes through the next stage where it offers diverse responses and a human annotator ranks it from the most appropriate to wrongâ€”training the system to compare.
